{"cells":[{"cell_type":"markdown","source":["# 03a - Parallel Switch-In Load Into Partitioned Table\n\nIf you have to load data into a table that is also actively used by users, you cannot just run a bulk copy operation on such table. If you plan to use `tableLock` option, users will not be able to access data for the whole duration of the bulk load. Even if you don't plan to use `tableLock` option, you will still impact and interfere with conccurrent operations running on the table partition.\n\nThe solution is simple: load another table instead, and then \"switch-in\" that table into the target one. More details on this pattern can be found in [this post](https://www.cathrinewilhelmsen.net/2015/04/19/table-partitioning-in-sql-server-partition-switching/) written by the Data Platform MVP Cathrine Wilhelmsen. \n\nBeside improving concurrency during bulk load operation, you also have another benefit that can be very useful. Without this pattern is usually better to load the table with indexes already created, as for very big table, creating an index can completely drain all the resources avaiable to your Azure SQL database. By using this tecnique you are actually using a \"divide-et-impera\" approach, so that you can load data into a staging table with no indexes, where you'll have the best load performance possible, and then create the needed index later, without the problem of resource exhaustion"],"metadata":{}},{"cell_type":"markdown","source":["Partitions in Databricks vs Partition in Azure SQL\n\n// https://medium.com/@mrpowers/managing-spark-partitions-with-coalesce-and-repartition-4050c57ad5c4"],"metadata":{}},{"cell_type":"markdown","source":["## Create support function\nTo be able to execute a switch-in load, parallel load must be managed manually, as T-SQL code must be execute before and after each Azure SQL partition has been loaded bia bulk load operation. By using the [tecnique explained in the official Databricks documentation](https://docs.databricks.com/notebooks/notebook-workflows.html#api) it is possibile to execute a notebook in parallel, but implementing the followign function."],"metadata":{}},{"cell_type":"code","source":["import scala.concurrent.{Future, Await}\nimport scala.concurrent.duration._\nimport scala.util.control.NonFatal\n\ncase class NotebookData(path: String, timeout: Int, parameters: Map[String, String] = Map.empty[String, String])\n\ndef parallelNotebooks(notebooks: Seq[NotebookData]): Future[Seq[String]] = {\n  import scala.concurrent.{Future, blocking, Await}\n  import java.util.concurrent.Executors\n  import scala.concurrent.ExecutionContext\n  import com.databricks.WorkflowException\n\n  val numNotebooksInParallel = 4 \n  // If you create too many notebooks in parallel the driver may crash when you submit all of the jobs at once. \n  // This code limits the number of parallel notebooks.\n  implicit val ec = ExecutionContext.fromExecutor(Executors.newFixedThreadPool(numNotebooksInParallel))\n  val ctx = dbutils.notebook.getContext()\n  \n  Future.sequence(\n    notebooks.map { notebook => \n      Future {\n        dbutils.notebook.setContext(ctx)\n        if (notebook.parameters.nonEmpty)\n          dbutils.notebook.run(notebook.path, notebook.timeout, notebook.parameters)\n        else\n          dbutils.notebook.run(notebook.path, notebook.timeout)\n      }\n      .recover {\n        case NonFatal(e) => s\"ERROR: ${e.getMessage}\"\n      }\n    }\n  )\n}"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">import scala.concurrent.{Future, Await}\nimport scala.concurrent.duration._\nimport scala.util.control.NonFatal\ndefined class NotebookData\nparallelNotebooks: (notebooks: Seq[NotebookData])scala.concurrent.Future[Seq[String]]\n</div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["## Run Parallel Load"],"metadata":{}},{"cell_type":"markdown","source":["Create a Sequence with Azure SQL partitions to be loaded is stored"],"metadata":{}},{"cell_type":"code","source":["import spark.implicits._\nimport org.apache.spark.sql._\n\ncase class partitionToProcess(partitionKey:Int)\n\nval ptp = Seq(\n    partitionToProcess(199702),\n    partitionToProcess(199703),\n    partitionToProcess(199705)\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">import spark.implicits._\nimport org.apache.spark.sql._\ndefined class partitionToProcess\nptp: Seq[partitionToProcess] = List(partitionToProcess(199702), partitionToProcess(199703), partitionToProcess(199705))\n</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["Execute in parallel several instances of the notebook that load a specific partition, using a different partition key for each instance"],"metadata":{}},{"cell_type":"code","source":["import scala.concurrent.Await\nimport scala.concurrent.duration._\nimport scala.language.postfixOps\n\nval timeOut = 600 // seconds\n\nval notebooks = ptp.map { \n  p => NotebookData(\"./03b-parallel-switch-in-load-into-partitioned-table-single\", \n                    timeOut, \n                    Map(\"partitionKey\" -> p.partitionKey.toString)\n                   )\n}\n\nval res = parallelNotebooks(notebooks)\n\nAwait.result(res, (timeOut * ptp.size seconds)) // this is a blocking call.\n\nres.value"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":10}],"metadata":{"name":"03a-parallel-switch-in-load-into-partitioned-table-all","notebookId":964636935775876},"nbformat":4,"nbformat_minor":0}