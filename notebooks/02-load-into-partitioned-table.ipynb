{"cells":[{"cell_type":"markdown","source":["# 02 - Load data into an Azure SQL partitioned table\n\nAzure SQL supports [table and index partitioning](https://docs.microsoft.com/en-us/sql/relational-databases/partitions/partitioned-tables-and-indexes). If a table is partitioned, data can be loaded in parallel without the need to put a lock on the entire table. In order to allow parallel partitions to be loaded, the source RDD/DataFrame/Dataset and the target Azure SQL table *MUST* have compatible partitions, which means that one RDD partition ends up exactly in one or more than one Azure SQL partitions, and those are not used by other RDD partitions.\n\nWhen table is partitioned, data *can* be bulk loaded in parallel also if there are indexes on the table. Especially on very large databases this is the recommended approach. The bulk load process will be slower, but you'll not need to create indexes after having loaded the data. Creation of indexes on huge, already loaded, tables is a very expensive operation that you would like to avoid if possibile.\n\nThe sample is using the new sql-spark-connector (https://github.com/microsoft/sql-spark-connector). The new connector must be manually installed by importing the .jar file (available in GitHub repo's releases) into the cluster.\nAnytime we mention \"row-store\" indexes, we mean an index that is not using the [column-store layout](https://docs.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-overview) to store its data.\n\nIn this notebook there are three samples\n\n- Load data into a partitioned table with row-store indexes\n- Load data into a partitioned table with columns-store indexes\n\nDatabricks supported versions: Spark 2.4.5 and Scala 2.11"],"metadata":{}},{"cell_type":"markdown","source":["## Setup"],"metadata":{}},{"cell_type":"markdown","source":["Define variables used thoughout the script. Azure Key Value has been used to securely store sensitive data. More info here: [Create an Azure Key Vault-backed secret scope](https://docs.microsoft.com/en-us/azure/databricks/security/secrets/secret-scopes#--create-an-azure-key-vault-backed-secret-scope)"],"metadata":{}},{"cell_type":"code","source":["val scope = \"key-vault-secrets\"\n\nval storageAccount = \"dmstore2\";\nval storageKey = dbutils.secrets.get(scope, \"dmstore2-2\");\n\nval server = dbutils.secrets.get(scope, \"srv001\").concat(\".database.windows.net\");\nval database = dbutils.secrets.get(scope, \"db001\");\nval user = dbutils.secrets.get(scope, \"dbuser001\");\nval password = dbutils.secrets.get(scope, \"dbpwd001\");\nval table = \"dbo.LINEITEM_LOADTEST\"\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">scope: String = key-vault-secrets\nstorageAccount: String = dmstore2\nstorageKey: String = [REDACTED]\nserver: String = [REDACTED].database.windows.net\ndatabase: String = [REDACTED]\nuser: String = [REDACTED]\npassword: String = [REDACTED]\ntable: String = dbo.LINEITEM_LOADTEST\n</div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["Configure Spark to access Azure Blob Store"],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(s\"fs.azure.account.key.$storageAccount.blob.core.windows.net\", storageKey);"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["Load the Parquet file generated in `00-create-parquet-file` notebook that contains LINEITEM data partitioned by Year and Month"],"metadata":{}},{"cell_type":"code","source":["val li = spark.read.parquet(s\"wasbs://tpch@$storageAccount.blob.core.windows.net/10GB/parquet/lineitem\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">li: org.apache.spark.sql.DataFrame = [L_ORDERKEY: int, L_PARTKEY: int ... 15 more fields]\n</div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["Loaded data is split in 20 dataframe partitions"],"metadata":{}},{"cell_type":"code","source":["li.rdd.getNumPartitions"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res2: Int = 20\n</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["Show schema of loaded data"],"metadata":{}},{"cell_type":"code","source":["li.printSchema"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- L_ORDERKEY: integer (nullable = true)\n-- L_PARTKEY: integer (nullable = true)\n-- L_SUPPKEY: integer (nullable = true)\n-- L_LINENUMBER: integer (nullable = true)\n-- L_QUANTITY: decimal(15,2) (nullable = true)\n-- L_EXTENDEDPRICE: decimal(15,2) (nullable = true)\n-- L_DISCOUNT: decimal(15,2) (nullable = true)\n-- L_TAX: decimal(15,2) (nullable = true)\n-- L_RETURNFLAG: string (nullable = true)\n-- L_LINESTATUS: string (nullable = true)\n-- L_SHIPDATE: date (nullable = true)\n-- L_COMMITDATE: date (nullable = true)\n-- L_RECEIPTDATE: date (nullable = true)\n-- L_SHIPINSTRUCT: string (nullable = true)\n-- L_SHIPMODE: string (nullable = true)\n-- L_COMMENT: string (nullable = true)\n-- L_PARTITION_KEY: integer (nullable = true)\n\n</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["All columns are shown as nullable, even if they were originally set to NOT NULL, so we will need to fix this to make sure data can be loaded correctly. \n\nSchema needs to be defined explicitly as connector is very sensitive to nullability, as per the following issue [Nullable column mismatch between Spark DataFrame & SQL Table Error](\nhttps://github.com/microsoft/sql-spark-connector/issues/5), so we need to explicity create the schema and apply it to the loaded data"],"metadata":{}},{"cell_type":"code","source":["import org.apache.spark.sql.types._\n\nval schema = StructType(\n    StructField(\"L_ORDERKEY\", IntegerType, false) ::\n    StructField(\"L_PARTKEY\", IntegerType, false) ::\n    StructField(\"L_SUPPKEY\", IntegerType, false) ::  \n    StructField(\"L_LINENUMBER\", IntegerType, false) ::\n    StructField(\"L_QUANTITY\", DecimalType(15,2), false) ::\n    StructField(\"L_EXTENDEDPRICE\", DecimalType(15,2), false) ::\n    StructField(\"L_DISCOUNT\", DecimalType(15,2), false) ::\n    StructField(\"L_TAX\", DecimalType(15,2), false) ::\n    StructField(\"L_RETURNFLAG\", StringType, false) ::\n    StructField(\"L_LINESTATUS\", StringType, false) ::\n    StructField(\"L_SHIPDATE\", DateType, false) ::\n    StructField(\"L_COMMITDATE\", DateType, false) ::\n    StructField(\"L_RECEIPTDATE\", DateType, false) ::\n    StructField(\"L_SHIPINSTRUCT\", StringType, false) ::  \n    StructField(\"L_SHIPMODE\", StringType, false) ::  \n    StructField(\"L_COMMENT\", StringType, false) ::  \n    StructField(\"L_PARTITION_KEY\", IntegerType, false) ::  \n    Nil)\n    \n  val li2 = spark.createDataFrame(li.rdd, schema)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">import org.apache.spark.sql.types._\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(L_ORDERKEY,IntegerType,false), StructField(L_PARTKEY,IntegerType,false), StructField(L_SUPPKEY,IntegerType,false), StructField(L_LINENUMBER,IntegerType,false), StructField(L_QUANTITY,DecimalType(15,2),false), StructField(L_EXTENDEDPRICE,DecimalType(15,2),false), StructField(L_DISCOUNT,DecimalType(15,2),false), StructField(L_TAX,DecimalType(15,2),false), StructField(L_RETURNFLAG,StringType,false), StructField(L_LINESTATUS,StringType,false), StructField(L_SHIPDATE,DateType,false), StructField(L_COMMITDATE,DateType,false), StructField(L_RECEIPTDATE,DateType,false), StructField(L_SHIPINSTRUCT,StringType,false), StructField(L_SHIPMODE,StringType,false), StructField(L_COMMENT,StringType,false), StructField(L_PARTITION_KEY,IntegerType,false))\nli2: org.apache.spark.sql.DataFrame = [L_ORDERKEY: int, L_PARTKEY: int ... 15 more fields]\n</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["Make sure you create on your Azure SQL the following LINEITEM table, partitioned by L_PARTITION_KEY:\n\n```sql\ncreate partition function pf_LINEITEM(int)\nas range left for values \n(\n\t199201,199202,199203,199204,199205,199206,199207,199208,199209,199210,199211,199212,\n\t199301,199302,199303,199304,199305,199306,199307,199308,199309,199310,199311,199312,\n\t199401,199402,199403,199404,199405,199406,199407,199408,199409,199410,199411,199412,\n\t199501,199502,199503,199504,199505,199506,199507,199508,199509,199510,199511,199512,\n\t199601,199602,199603,199604,199605,199606,199607,199608,199609,199610,199611,199612,\n\t199701,199702,199703,199704,199705,199706,199707,199708,199709,199710,199711,199712,\n\t199801,199802,199803,199804,199805,199806,199807,199808,199809,199810\n);\n\ncreate partition scheme ps_LINEITEM\nas partition pf_LINEITEM\nall to ([Primary])\n;\n\ncreate table [dbo].[LINEITEM_LOADTEST]\n(\n\t[L_ORDERKEY] [int] not null,\n\t[L_PARTKEY] [int] not null,\n\t[L_SUPPKEY] [int] not null,\n\t[L_LINENUMBER] [int] not null,\n\t[L_QUANTITY] [decimal](15, 2) not null,\n\t[L_EXTENDEDPRICE] [decimal](15, 2) not null,\n\t[L_DISCOUNT] [decimal](15, 2) not null,\n\t[L_TAX] [decimal](15, 2) not null,\n\t[L_RETURNFLAG] [char](1) not null,\n\t[L_LINESTATUS] [char](1) not null,\n\t[L_SHIPDATE] [date] not null,\n\t[L_COMMITDATE] [date] not null,\n\t[L_RECEIPTDATE] [date] not null,\n\t[L_SHIPINSTRUCT] [char](25) not null,\n\t[L_SHIPMODE] [char](10) not null,\n\t[L_COMMENT] [varchar](44) not null,\n\t[L_PARTITION_KEY] [int] not null\n) on ps_LINEITEM([L_PARTITION_KEY])\n```"],"metadata":{}},{"cell_type":"markdown","source":["You can check that Azure SQL table is partitioned by running the following T-SQL command:\n\n```sql\nSELECT\n    schema_name(t.schema_id) as [schema_name],\n    t.[name] as table_name,\n    i.[name] as index_name,\n    ps.[partition_id],\n    ps.partition_number,\n    p.data_compression_desc,\n    i.[type_desc],    \n    ps.row_count,\n    (ps.used_page_count * 8.) / 1024. / 1024. as size_in_gb\nfrom\n    sys.dm_db_partition_stats as ps \ninner join  \n    sys.partitions as p on ps.partition_id = p.partition_id\ninner join\n    sys.tables as t on t.object_id = ps.object_id\ninner join\n    sys.indexes as i on ps.object_id = i.object_id and ps.index_id = i.index_id\nwhere\n    t.[name] = 'LINEITEM_LOADTEST' and t.[schema_id] = schema_id('dbo')\norder by\n    [schema_name], table_name, index_name, partition_number\n```"],"metadata":{}},{"cell_type":"markdown","source":["## Load data into a partitioned table with row-store indexes"],"metadata":{}},{"cell_type":"markdown","source":["On the target table create the Clustered Index and a couple of Non-Clustered Index:\n\n```sql\ncreate clustered index IXC on dbo.[LINEITEM_LOADTEST] ([L_COMMITDATE]) \non ps_LINEITEM([L_PARTITION_KEY]);\n\ncreate unique nonclustered index IX1 on dbo.[LINEITEM_LOADTEST] ([L_ORDERKEY], [L_LINENUMBER], [L_PARTITION_KEY]) \non ps_LINEITEM([L_PARTITION_KEY]);\n\ncreate nonclustered index IX2 on dbo.[LINEITEM_LOADTEST] ([L_PARTKEY], [L_PARTITION_KEY]) \non ps_LINEITEM([L_PARTITION_KEY]);\n```"],"metadata":{}},{"cell_type":"markdown","source":["As DataFrame and Azure SQL Table are both partitioned by L_PARTITION_KEY, there isn't much left to do and the connector will take care of everything for us. `tableLock` must be set to `false` to avoid table lock that will prevent parallel partitioned load. Thanks to partitions, acquired lockes will not interfere with each other."],"metadata":{}},{"cell_type":"code","source":["val url = s\"jdbc:sqlserver://$server;databaseName=$database;\"\n\nli2.write \n  .format(\"com.microsoft.sqlserver.jdbc.spark\") \n  .mode(\"overwrite\")   \n  .option(\"truncate\", \"true\") \n  .option(\"url\", url) \n  .option(\"dbtable\", \"dbo.LINEITEM_LOADTEST\") \n  .option(\"user\", user) \n  .option(\"password\", password) \n  .option(\"reliabilityLevel\", \"BEST_EFFORT\") \n  .option(\"tableLock\", \"false\") \n  .option(\"batchsize\", \"100000\") \n  .save()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["## Load data into a partitioned table with column-store index"],"metadata":{}},{"cell_type":"markdown","source":["Empty table if needed, to speed up index deletion\n\n```sql\ntruncate table dbo.[LINEITEM_LOADTEST];\n```\n\nDrop the previously create indexes if needed:\n```sql\ndrop index IXC on dbo.[LINEITEM_LOADTEST];\ndrop index IX1 on dbo.[LINEITEM_LOADTEST];\ndrop index IX2 on dbo.[LINEITEM_LOADTEST];\n```\n\nAnd the create a clustered columnstore index:\n\n```sql\ncreate clustered columnstore index IXCCS on dbo.[LINEITEM_LOADTEST]\non ps_LINEITEM([L_PARTITION_KEY]);\n```"],"metadata":{}},{"cell_type":"markdown","source":["Load data using [columnstore data loading best pratices](https://docs.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-data-loading-guidance), by loading 1048576 rows at time, to land directly into a compressed segment. Locking the table must be set to `false` to avoid locking. Data with be loaded in parallel, using as many as Apache Spark workers are available."],"metadata":{}},{"cell_type":"code","source":["val url = s\"jdbc:sqlserver://$server;databaseName=$database;\"\n\nli2.write \n  .format(\"com.microsoft.sqlserver.jdbc.spark\") \n  .mode(\"overwrite\")   \n  .option(\"truncate\", \"true\") \n  .option(\"url\", url) \n  .option(\"dbtable\", \"dbo.LINEITEM_LOADTEST\") \n  .option(\"user\", user) \n  .option(\"password\", password) \n  .option(\"reliabilityLevel\", \"BEST_EFFORT\") \n  .option(\"tableLock\", \"false\") \n  .option(\"batchsize\", \"1048576\") \n  .save()"],"metadata":{},"outputs":[],"execution_count":24}],"metadata":{"name":"02-load-into-partitioned-table","notebookId":1536696850337469},"nbformat":4,"nbformat_minor":0}