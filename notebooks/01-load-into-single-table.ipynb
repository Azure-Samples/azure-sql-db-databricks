{"cells":[{"cell_type":"markdown","source":["# 01 - Load data into an Azure SQL non-partitioned table\n\nThe sample is using the new sql-spark-connector (https://github.com/microsoft/sql-spark-connector). The new connector must be manually installed by importing the .jar file (available in GitHub repo's releases) into the cluster.\n\n## Notes on terminology\n\nThe term \"row-store\" is used to identify and index that is not using the [column-store layout](https://docs.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-overview) to store its data.\n\n## Samples\n\nIn this notebook there are three samples\n\n- Load data into a table without indexes\n- Load data into a table with row-store indexes\n- Load data into a table with columns-store indexes\n\n## Supported Azure Databricks Versions\n\nDatabricks supported versions: Spark 2.4.5 and Scala 2.11"],"metadata":{}},{"cell_type":"markdown","source":["## Setup"],"metadata":{}},{"cell_type":"markdown","source":["Define variables used thoughout the script. Azure Key Value has been used to securely store sensitive data. More info here: [Create an Azure Key Vault-backed secret scope](https://docs.microsoft.com/en-us/azure/databricks/security/secrets/secret-scopes#--create-an-azure-key-vault-backed-secret-scope)"],"metadata":{}},{"cell_type":"code","source":["val scope = \"key-vault-secrets\"\n\nval storageAccount = \"dmstore2\";\nval storageKey = dbutils.secrets.get(scope, \"dmstore2-2\");\n\nval server = dbutils.secrets.get(scope, \"srv001\").concat(\".database.windows.net\");\nval database = dbutils.secrets.get(scope, \"db001\");\nval user = dbutils.secrets.get(scope, \"dbuser001\");\nval password = dbutils.secrets.get(scope, \"dbpwd001\");\nval table = \"dbo.LINEITEM_LOADTEST\"\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">scope: String = key-vault-secrets\nstorageAccount: String = dmstore2\nstorageKey: String = [REDACTED]\nserver: String = [REDACTED].database.windows.net\ndatabase: String = [REDACTED]\nuser: String = [REDACTED]\npassword: String = [REDACTED]\ntable: String = dbo.LINEITEM_LOADTEST\n</div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["Configure Spark to access Azure Blob Store"],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(s\"fs.azure.account.key.$storageAccount.blob.core.windows.net\", storageKey);"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["Load the Parquet file generated in `00-create-parquet-file` notebook that contains LINEITEM data partitioned by Year and Month"],"metadata":{}},{"cell_type":"code","source":["val li = spark.read.parquet(s\"wasbs://tpch@$storageAccount.blob.core.windows.net/10GB/parquet/lineitem\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">li: org.apache.spark.sql.DataFrame = [L_ORDERKEY: int, L_PARTKEY: int ... 15 more fields]\n</div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["Loaded data is split in 20 dataframe partitions"],"metadata":{}},{"cell_type":"code","source":["li.rdd.getNumPartitions"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res2: Int = 20\n</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["Show schema of loaded data"],"metadata":{}},{"cell_type":"code","source":["li.printSchema"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- L_ORDERKEY: integer (nullable = true)\n-- L_PARTKEY: integer (nullable = true)\n-- L_SUPPKEY: integer (nullable = true)\n-- L_LINENUMBER: integer (nullable = true)\n-- L_QUANTITY: decimal(15,2) (nullable = true)\n-- L_EXTENDEDPRICE: decimal(15,2) (nullable = true)\n-- L_DISCOUNT: decimal(15,2) (nullable = true)\n-- L_TAX: decimal(15,2) (nullable = true)\n-- L_RETURNFLAG: string (nullable = true)\n-- L_LINESTATUS: string (nullable = true)\n-- L_SHIPDATE: date (nullable = true)\n-- L_COMMITDATE: date (nullable = true)\n-- L_RECEIPTDATE: date (nullable = true)\n-- L_SHIPINSTRUCT: string (nullable = true)\n-- L_SHIPMODE: string (nullable = true)\n-- L_COMMENT: string (nullable = true)\n-- L_PARTITION_KEY: integer (nullable = true)\n\n</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["All columns are shown as nullable, even if they were originally set to NOT NULL, so we will need to fix this to make sure data can be loaded correctly. Schema needs to be defined explicitly as connector is very sensitive to nullability, as per the following issue [Nullable column mismatch between Spark DataFrame & SQL Table Error](\nhttps://github.com/microsoft/sql-spark-connector/issues/5), so we need to explicity create the schema and apply it to the loaded data"],"metadata":{}},{"cell_type":"code","source":["import org.apache.spark.sql.types._\n\nval schema = StructType(\n    StructField(\"L_ORDERKEY\", IntegerType, false) ::\n    StructField(\"L_PARTKEY\", IntegerType, false) ::\n    StructField(\"L_SUPPKEY\", IntegerType, false) ::  \n    StructField(\"L_LINENUMBER\", IntegerType, false) ::\n    StructField(\"L_QUANTITY\", DecimalType(15,2), false) ::\n    StructField(\"L_EXTENDEDPRICE\", DecimalType(15,2), false) ::\n    StructField(\"L_DISCOUNT\", DecimalType(15,2), false) ::\n    StructField(\"L_TAX\", DecimalType(15,2), false) ::\n    StructField(\"L_RETURNFLAG\", StringType, false) ::\n    StructField(\"L_LINESTATUS\", StringType, false) ::\n    StructField(\"L_SHIPDATE\", DateType, false) ::\n    StructField(\"L_COMMITDATE\", DateType, false) ::\n    StructField(\"L_RECEIPTDATE\", DateType, false) ::\n    StructField(\"L_SHIPINSTRUCT\", StringType, false) ::  \n    StructField(\"L_SHIPMODE\", StringType, false) ::  \n    StructField(\"L_COMMENT\", StringType, false) ::  \n    StructField(\"L_PARTITION_KEY\", IntegerType, false) ::  \n    Nil)\n    \nval li2 = spark.createDataFrame(li.rdd, schema)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">import org.apache.spark.sql.types._\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(L_ORDERKEY,IntegerType,false), StructField(L_PARTKEY,IntegerType,false), StructField(L_SUPPKEY,IntegerType,false), StructField(L_LINENUMBER,IntegerType,false), StructField(L_QUANTITY,DecimalType(15,2),false), StructField(L_EXTENDEDPRICE,DecimalType(15,2),false), StructField(L_DISCOUNT,DecimalType(15,2),false), StructField(L_TAX,DecimalType(15,2),false), StructField(L_RETURNFLAG,StringType,false), StructField(L_LINESTATUS,StringType,false), StructField(L_SHIPDATE,DateType,false), StructField(L_COMMITDATE,DateType,false), StructField(L_RECEIPTDATE,DateType,false), StructField(L_SHIPINSTRUCT,StringType,false), StructField(L_SHIPMODE,StringType,false), StructField(L_COMMENT,StringType,false), StructField(L_PARTITION_KEY,IntegerType,false))\nli2: org.apache.spark.sql.DataFrame = [L_ORDERKEY: int, L_PARTKEY: int ... 15 more fields]\n</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["Now, make sure you create on your Azure SQL the following LINEITEM table:\n```sql\ncreate table [dbo].[LINEITEM_LOADTEST]\n(\n\t[L_ORDERKEY] [int] not null,\n\t[L_PARTKEY] [int] not null,\n\t[L_SUPPKEY] [int] not null,\n\t[L_LINENUMBER] [int] not null,\n\t[L_QUANTITY] [decimal](15, 2) not null,\n\t[L_EXTENDEDPRICE] [decimal](15, 2) not null,\n\t[L_DISCOUNT] [decimal](15, 2) not null,\n\t[L_TAX] [decimal](15, 2) not null,\n\t[L_RETURNFLAG] [char](1) not null,\n\t[L_LINESTATUS] [char](1) not null,\n\t[L_SHIPDATE] [date] not null,\n\t[L_COMMITDATE] [date] not null,\n\t[L_RECEIPTDATE] [date] not null,\n\t[L_SHIPINSTRUCT] [char](25) not null,\n\t[L_SHIPMODE] [char](10) not null,\n\t[L_COMMENT] [varchar](44) not null,\n\t[L_PARTITION_KEY] [int] not null\n) \n```"],"metadata":{}},{"cell_type":"markdown","source":["## Load data into a table with no indexes\n\nIn Azure SQL terminology an Heap is a table with no clustered index. In this sample we'll load data into a table that as no index (clustered or non-clustered) as is not partitioned. This is the simplest scenario possibile and allows parallel load of data.\n\n### Note:\nParallel load *cannot* happen if you have row-store indexes on the table. If you want to bulk load data in parallel into a table that has row-store indexes, you must use partitioning. If you are planning to add indexes to your table, and data to be loaded in the table is in the terabyte range, you want to use partitioing and have indexes created before bulk loading data into Azure SQL, as otherwise creating index once the table is already loaded will use a significat amout of resources."],"metadata":{}},{"cell_type":"markdown","source":["To enable parallel load the option `tableLock` must be set to `true`. This will prevent any other access to the table, other then the one done for performing the bulk load operations."],"metadata":{}},{"cell_type":"code","source":["val url = s\"jdbc:sqlserver://$server;databaseName=$database;\"\n\nli2.write \n  .format(\"com.microsoft.sqlserver.jdbc.spark\") \n  .mode(\"overwrite\")   \n  .option(\"truncate\", \"true\") \n  .option(\"url\", url) \n  .option(\"dbtable\", \"dbo.LINEITEM_LOADTEST\") \n  .option(\"user\", user) \n  .option(\"password\", password) \n  .option(\"reliabilityLevel\", \"BEST_EFFORT\") \n  .option(\"tableLock\", \"true\") \n  .option(\"batchsize\", \"100000\")   \n  .save()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">url: String = jdbc:sqlserver://[REDACTED].database.windows.net;databaseName=[REDACTED];\n</div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["## Load data into a table with row-store indexes\n\nIf table is not partitioned, there are no options to bulk load data in parallel into the desired table. The only way to avoid locking and deadlocks is to load everything by serializing the bulk load operations. As you can expect, performance won't be the optimal."],"metadata":{}},{"cell_type":"markdown","source":["Create the following index on the table\n```sql\ncreate clustered index IXC on dbo.[LINEITEM_LOADTEST] ([L_COMMITDATE]);\n\ncreate unique nonclustered index IX1 on dbo.[LINEITEM_LOADTEST] ([L_ORDERKEY], [L_LINENUMBER]);\n\ncreate nonclustered index IX2 on dbo.[LINEITEM_LOADTEST] ([L_PARTKEY]); \n```"],"metadata":{}},{"cell_type":"markdown","source":["Load data by coalescing all dataframe partitions into just one"],"metadata":{}},{"cell_type":"code","source":["val url = s\"jdbc:sqlserver://$server;databaseName=$database;\"\n\nli2.coalesce(1).write \n  .format(\"com.microsoft.sqlserver.jdbc.spark\") \n  .mode(\"overwrite\")   \n  .option(\"truncate\", \"true\") \n  .option(\"url\", url) \n  .option(\"dbtable\", \"dbo.LINEITEM_LOADTEST\") \n  .option(\"user\", user) \n  .option(\"password\", password) \n  .option(\"reliabilityLevel\", \"BEST_EFFORT\") \n  .option(\"tableLock\", \"false\") \n  .option(\"batchsize\", \"100000\")   \n  .save()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["## Load data into a table with (only) column-store indexes\n\nIf a table has only column-store indexes, data load can happen in parallel, as there is no sorting needed."],"metadata":{}},{"cell_type":"markdown","source":["Empty table if needed, to speed up index deletion\n\n```sql\ntruncate table dbo.[LINEITEM_LOADTEST];\n```\n\nDrop the previously create indexes if needed:\n```sql\ndrop index IXC on dbo.[LINEITEM_LOADTEST];\ndrop index IX1 on dbo.[LINEITEM_LOADTEST];\ndrop index IX2 on dbo.[LINEITEM_LOADTEST];\n```\n\nAnd the create a clustered columnstore index:\n\n```sql\ncreate clustered columnstore index IXCCS on dbo.[LINEITEM_LOADTEST]\n```"],"metadata":{}},{"cell_type":"markdown","source":["Load data using [columnstore data loading best pratices](https://docs.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-data-loading-guidance), by loading 1048576 rows at time, to land directly into a compressed segment. `tableLock` options must be set to `false` to avoid table lock that will prevent parallel load. Data with be loaded in parallel, using as many as Apache Spark workers are available."],"metadata":{}},{"cell_type":"code","source":["val url = s\"jdbc:sqlserver://$server;databaseName=$database;\"\n\nli2.write \n  .format(\"com.microsoft.sqlserver.jdbc.spark\") \n  .mode(\"overwrite\")   \n  .option(\"truncate\", \"true\") \n  .option(\"url\", url) \n  .option(\"dbtable\", \"dbo.LINEITEM_LOADTEST\") \n  .option(\"user\", user) \n  .option(\"password\", password) \n  .option(\"reliabilityLevel\", \"BEST_EFFORT\") \n  .option(\"tableLock\", \"false\") \n  .option(\"batchsize\", \"1048576\")   \n  .save()"],"metadata":{},"outputs":[],"execution_count":26}],"metadata":{"name":"01-load-into-single-table","notebookId":1331848450253174},"nbformat":4,"nbformat_minor":0}